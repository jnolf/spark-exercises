{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4337de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b92e0",
   "metadata": {},
   "source": [
    "#### 1.) Read the case, department, and source data into their own spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02a489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "case = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/case.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e494960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/dept.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c45cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/source.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c1ade0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case.csv\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date|SLA_due_date|case_late|      num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29|9/26/20 0:42|       NO| -998.5087616000001|        YES|Field Operations|        Stray Animal|      999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|               5|\n",
      "|1014127333|     1/1/18 0:46|     1/3/18 8:11| 1/5/18 8:30|       NO|-2.0126041669999997|        YES|     Storm Water|Removal Of Obstru...|4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|               3|\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "dept.csv\n",
      "+---------------+--------------------+----------------------+-------------------+\n",
      "|  dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+---------------+--------------------+----------------------+-------------------+\n",
      "|311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|          Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "+---------------+--------------------+----------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "source.csv\n",
      "+---------+----------------+\n",
      "|source_id| source_username|\n",
      "+---------+----------------+\n",
      "|   100137|Merlene Blodgett|\n",
      "|   103582|     Carmen Cura|\n",
      "+---------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('case.csv')\n",
    "case.show(2)\n",
    "print('dept.csv')\n",
    "dept.show(2) \n",
    "print('source.csv')\n",
    "source.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74764d8",
   "metadata": {},
   "source": [
    "Let's see how writing to the local disk works in spark:\n",
    "\n",
    "Write the code necessary to store the source data in both csv and json format, store these as `sources_csv` and `sources_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09a7fcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dept.write.json('dept_json', mode='overwrite')\n",
    "source.write.json('source_json', mode='overwrite')\n",
    "case.write.json('case_json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d606c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
